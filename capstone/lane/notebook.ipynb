{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Detection Using Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying lanes on the road is a common task performed by all human drivers to ensure their vehicles are within lane constraints when driving, so as to make sure traffic is smooth and minimize chances of collisions with other cars in nearby lanes.\n",
    "\n",
    "Similarly, it is a critical task for an autonomous vehicle to perform. It turns out that recognising lane markings on roads is possible using well known computer vision techniques. We will cover how to use various techniques to identify and draw the inside road lanes, lane curvature, and even estimate the vehicle position relative to the lane.\n",
    "\n",
    "This is project 4 of Term 1 of the Udacity Self Driving Car Engineer Nanodegree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration And Undistortion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step we will take is to find the calibration matrix, along with distortion coefficient for the camera that was used to take pictures of the road. This is necessary because the convex shape of camera lenses curves light rays as the enter the pinhole, therefore causing distortions to the real image. Therefore lines that are straight in the real world may not be anymore on our photos. \n",
    "\n",
    "To compute the camera the transformation matrix and distortion coefficients, we use a multiple pictures of a _chessboard_ on a flat surface taken **by the same camera**. OpenCV has a convenient method called [findChessboardCorners](http://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#cv2.findChessboardCorners) that will identify the points where black and white squares intersect and reverse engineer the distorsion matrix this way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import utils; reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_dir = \"camera_cal\"\n",
    "test_imgs_dir = \"test_images\"\n",
    "output_imgs_dir = \"output_images\"\n",
    "output_videos_dir = \"output_videos\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Chessboard Corners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must determine the number of inner corners horizontally and vertically for our sample chessboard pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get all our calibration image paths\n",
    "cal_imgs_paths = glob.glob(calibration_dir + \"/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's the first chessboard image to see what it looks like\n",
    "cal_img_path = cal_imgs_paths[11]\n",
    "cal_img = load_image(cal_img_path)\n",
    "plt.imshow(cal_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our chessboard has *9* inner corners in the x direction, and *6* in the y direction. We will use these as parameters to *findChessboardCorners()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cx = 9\n",
    "cy = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We submit grayscale images to _findChessboardCorners_ so we must therefore define a utility method for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findChessboardCorners(img, nx, ny):\n",
    "    \"\"\"\n",
    "    Finds the chessboard corners of the supplied image (must be grayscale)\n",
    "    nx and ny parameters respectively indicate the number of inner corners in the x and y directions\n",
    "    \"\"\"\n",
    "    return cv2.findChessboardCorners(img, (nx, ny), None)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showChessboardCorners(img, nx, ny, ret, corners):\n",
    "    \"\"\"\n",
    "    Draws the chessboard corners of a given image\n",
    "    nx and ny parameters respectively indicate the number of inner corners in the x and y directions\n",
    "    ret and corners should represent the results from cv2.findChessboardCorners()\n",
    "    \"\"\"\n",
    "    c_img = cv2.drawChessboardCorners(img, (nx, ny), corners, ret)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, corners = findChessboardCorners(to_grayscale(cal_img), cx, cy)\n",
    "showChessboardCorners(cal_img, cx, cy, ret, corners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that corners are very well identified. Next we identify image and object points to calibrate the camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Undistortion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Image And Object Points\n",
    "\n",
    "We find the mappings for coordinates of the images in 2D space (i.e. *image points*) to those of the undistorted image in the real-world (i.e. *object points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findImgObjPoints(imgs_paths, nx, ny):\n",
    "    \"\"\"\n",
    "    Returns the objects and image points computed for a set of chessboard pictures taken from the same camera\n",
    "    nx and ny parameters respectively indicate the number of inner corners in the x and y directions\n",
    "    \"\"\"\n",
    "    objpts = []\n",
    "    imgpts = []\n",
    "    \n",
    "    # Pre-compute what our object points in the real world should be (the z dimension is 0 as we assume a flat surface)\n",
    "    objp = np.zeros((nx * ny, 3), np.float32)\n",
    "    objp[:, :2] = np.mgrid[0:nx, 0:ny].T.reshape(-1, 2)\n",
    "    \n",
    "    for img_path in imgs_paths:\n",
    "        img = load_image(img_path)\n",
    "        gray = to_grayscale(img)\n",
    "        ret, corners = findChessboardCorners(gray, nx, ny)\n",
    "        \n",
    "        if ret:\n",
    "            # Found the corners of an image\n",
    "            imgpts.append(corners)\n",
    "            # Add the same object point since they don't change in the real world\n",
    "            objpts.append(objp)\n",
    "    \n",
    "    return objpts, imgpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts, ipts = findImgObjPoints(cal_imgs_paths, cx, cy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Calibration Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort_image(img, objpts, imgpts):\n",
    "    \"\"\"\n",
    "    Returns an undistorted image\n",
    "    The desired object and image points must also be supplied to this function\n",
    "    \"\"\"\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpts, imgpts, to_grayscale(img).shape[::-1], None, None)\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return undist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_img_example = load_image(cal_imgs_paths[0])\n",
    "cal_img_undist = undistort_image(cal_img_example, opts, ipts)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,7))\n",
    "ax[0].imshow(cal_img_example)\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].set_title(\"Distorted Image\")\n",
    "\n",
    "ax[1].imshow(cal_img_undist)\n",
    "ax[1].axis(\"off\")\n",
    "ax[1].set_title(\"Undistorted Image\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the undistortion step indeed works well. Let's apply the undistortion step to our test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undistorting Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_paths = glob.glob(test_imgs_dir + \"/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_names = np.asarray(list(map(lambda img_path: img_path.split(\"/\")[-1].split(\".\")[0], test_imgs_paths)))\n",
    "undist_test_img_names = np.asarray(list(map(lambda img_name: \"{0}{1}\".format(\"undistorted_\", img_name), test_img_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = np.asarray(list(map(lambda img_path: load_image(img_path), test_imgs_paths)))\n",
    "undist_test_imgs = np.asarray(list(map(lambda img: undistort_image(img, opts, ipts), test_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_and_undist = np.asarray(list(zip(test_imgs, undist_test_imgs)))\n",
    "test_img_and_undist_names = np.asarray(list(zip(test_img_names, undist_test_img_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(test_img_and_undist, test_img_and_undist_names, \"Test Images vs Undistored Images\", fig_size=(12, 20), cols=2, show_ticks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample above shows original next to undistored images. We can see that on the undistorted images, the front of the car, especially at the sides, is less visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply color and edge  thresholding in this section to better detect the lines, and make it easier to find the polynomial that best describes our left and right lanes later. \n",
    "\n",
    "We start with first exploring which color spaces we should adopt to increase our chances of detecting the lanes and facilitating the task of the gradient thresholding step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_img(img, channel, thres=(0, 255)):\n",
    "    \"\"\"\n",
    "    Applies a threshold mask to the input image\n",
    "    \"\"\"\n",
    "    img_ch = img[:,:,channel]\n",
    "    if thres is None:  \n",
    "        return img_ch\n",
    "    \n",
    "    mask_ch = np.zeros_like(img_ch)\n",
    "    mask_ch[ (thres[0] <= img_ch) & (thres[1] >= img_ch) ] = 1\n",
    "    return mask_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = test_imgs_paths[7]\n",
    "test_img = load_image(test_img_path)\n",
    "undistorted_test_img = undistort_image(test_img, opts, ipts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Thresholding\n",
    "\n",
    "We experiment with different color spaces and try out some thresolding values to identify the best parameters and most suitable color space and channel to pick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_comp = np.asarray([[threshold_img(undistorted_test_img, 0, thres=None), threshold_img(undistorted_test_img, 1, thres=None), threshold_img(undistorted_test_img, 2, thres=None)]])\n",
    "rgb_lbs = np.asarray([[\"Red Channel\", \"Green Channel\", \"Blue Channel\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_test_img = to_hls(undistorted_test_img)\n",
    "hls_comp = np.asarray([[threshold_img(hls_test_img, 0, thres=None), threshold_img(hls_test_img, 1, thres=None), threshold_img(hls_test_img, 2, thres=None)]])\n",
    "hls_lbs = np.asarray([[\"Hue Channel\", \"Lightness Channel\", \"Saturation Channel\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv_test_img = to_hsv(undistorted_test_img)\n",
    "hsv_comp = np.asarray([[threshold_img(hsv_test_img, 0, thres=None), threshold_img(hsv_test_img, 1, thres=None), threshold_img(hsv_test_img, 2, thres=None)]])\n",
    "hsv_lbs = np.asarray([[\"Hue Channel\", \"Saturation Channel\", \"Value Channel\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_test_img = to_lab(undistorted_test_img)\n",
    "lab_comp = np.asarray([[threshold_img(lab_test_img, 0, thres=None), threshold_img(lab_test_img, 1, thres=None), threshold_img(lab_test_img, 2, thres=None)]])\n",
    "lab_lbs = np.asarray([[\"Lightness Channel\", \"Green-Red (A) Channel\", \"Blue-Yellow (B) Channel\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_spaces_comps = np.concatenate((rgb_comp, hls_comp, hsv_comp, lab_comp))\n",
    "color_spaces_lbs = np.concatenate((rgb_lbs, hls_lbs, hsv_lbs, lab_lbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(color_spaces_comps, color_spaces_lbs, \"Color Channels: RGB - HLS - HSV - LAB\", cols=3, fig_size=(15, 10), show_ticks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the RGB components, we see that the blue channel is worst at identifying yellow lines, while the red channel seems to give best results.\n",
    "\n",
    "For HLS and HSV, the hue channel produces an extremely noisy output, while the saturation channel of HLS seems to give the strong results; better than HSV's saturation channel. conversely, HSV's value channel is giving a very clear grayscale-ish image, especially on the yellow line, much better than HLS' lightness channel.\n",
    "\n",
    "Lastly, LAB's A channel is not doing a great job, while it's B channel is strong at identifying the yellow line. But it is the lightness channel that shines (no pun intended) at identify both yellow and white lines.\n",
    "\n",
    "At this stage, we are faced with various choices that have pros and cons. Our goal here is to find the right thresholds on a given color channel to highlight yellow and white lines of the lane. There are actually many ways we could achieve this result, but **we choose to use HLS because we already know how to set thresholds for yellow and white lane lines from [Project 1](https://github.com/kenshiro-o/CarND-LaneLines-P1)** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hls_white_yellow_binary(rgb_img):\n",
    "    \"\"\"\n",
    "    Returns a binary thresholded image produced retaining only white and yellow elements on the picture\n",
    "    The provided image should be in RGB format\n",
    "    \"\"\"\n",
    "    hls_img = to_hls(rgb_img)\n",
    "    \n",
    "    # Compute a binary thresholded image where yellow is isolated from HLS components\n",
    "    img_hls_yellow_bin = np.zeros_like(hls_img[:,:,0])\n",
    "    img_hls_yellow_bin[((hls_img[:,:,0] >= 15) & (hls_img[:,:,0] <= 35))\n",
    "                 & ((hls_img[:,:,1] >= 30) & (hls_img[:,:,1] <= 204))\n",
    "                 & ((hls_img[:,:,2] >= 115) & (hls_img[:,:,2] <= 255))                \n",
    "                ] = 1\n",
    "    \n",
    "    # Compute a binary thresholded image where white is isolated from HLS components\n",
    "    img_hls_white_bin = np.zeros_like(hls_img[:,:,0])\n",
    "    img_hls_white_bin[((hls_img[:,:,0] >= 0) & (hls_img[:,:,0] <= 255))\n",
    "                 & ((hls_img[:,:,1] >= 200) & (hls_img[:,:,1] <= 255))\n",
    "                 & ((hls_img[:,:,2] >= 0) & (hls_img[:,:,2] <= 255))                \n",
    "                ] = 1\n",
    "    \n",
    "    # Now combine both\n",
    "    img_hls_white_yellow_bin = np.zeros_like(hls_img[:,:,0])\n",
    "    img_hls_white_yellow_bin[(img_hls_yellow_bin == 1) | (img_hls_white_bin == 1)] = 1\n",
    "\n",
    "    return img_hls_white_yellow_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undistorted_yellow_white_hls_img_bin = compute_hls_white_yellow_binary(undistorted_test_img)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,7))\n",
    "ax[0].imshow(undistorted_test_img)\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].set_title(\"Undistorted Image\")\n",
    "\n",
    "ax[1].imshow(undistorted_yellow_white_hls_img_bin, cmap='gray')\n",
    "ax[1].axis(\"off\")\n",
    "ax[1].set_title(\"HLS Color Thresholded Image\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, our HLS color thresholding achieve great results on the image. The thresholding somewhat struggles a little with the shadow of the tree on the yellow line further up ahead. We believe gradient thresholding can help in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Thresholding Via Sobel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [Sobel](https://en.wikipedia.org/wiki/Sobel_operator) operator to identify _gradients_, that is change in _color intensity_ in the image. Higher values would denote strong gradients, and therefore sharp changes in color.\n",
    "\n",
    "We have decided to use LAB's L channel as our single-channel image to serve as input to the sobel functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L channel is index 0 of the color components\n",
    "undist_test_img_gray = to_lab(undistorted_test_img)[:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sobel In X or Y Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_sobel(gray_img, x_dir=True, kernel_size=3, thres=(0, 255)):\n",
    "    \"\"\"\n",
    "    Applies the sobel operator to a grayscale-like (i.e. single channel) image in either horizontal or vertical direction\n",
    "    The function also computes the asbolute value of the resulting matrix and applies a binary threshold\n",
    "    \"\"\"\n",
    "    sobel = cv2.Sobel(gray_img, cv2.CV_64F, 1, 0, ksize=kernel_size) if x_dir else cv2.Sobel(gray_img, cv2.CV_64F, 0, 1, ksize=kernel_size) \n",
    "    sobel_abs = np.absolute(sobel)\n",
    "    sobel_scaled = np.uint8(255 * sobel / np.max(sobel_abs))\n",
    "    \n",
    "    gradient_mask = np.zeros_like(sobel_scaled)\n",
    "    gradient_mask[(thres[0] <= sobel_scaled) & (sobel_scaled <= thres[1])] = 1\n",
    "    return gradient_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobx_3x3_thres = np.asarray([[abs_sobel(undist_test_img_gray, thres=(20, 120)), abs_sobel(undist_test_img_gray, thres=(50, 150)), abs_sobel(undist_test_img_gray, thres=(80, 200))]])\n",
    "sobx_7x7_thres = np.asarray([[abs_sobel(undist_test_img_gray, kernel_size=7, thres=(20, 120)), abs_sobel(undist_test_img_gray, kernel_size=7, thres=(50, 150)), abs_sobel(undist_test_img_gray, kernel_size=7, thres=(80, 200))]])\n",
    "sobx_11x11_thres = np.asarray([[abs_sobel(undist_test_img_gray, kernel_size=11, thres=(20, 120)), abs_sobel(undist_test_img_gray, kernel_size=11, thres=(50, 150)), abs_sobel(undist_test_img_gray, kernel_size=11, thres=(80, 200))]])\n",
    "sobx_15x15_thres = np.asarray([[abs_sobel(undist_test_img_gray, kernel_size=15, thres=(20, 120)), abs_sobel(undist_test_img_gray, kernel_size=15, thres=(50, 150)), abs_sobel(undist_test_img_gray, kernel_size=15, thres=(80, 200))]])\n",
    "\n",
    "sobx_3x3_thres_lbs = np.asarray([[\"3x3 - Threshold (20,120)\", \"3x3 - Threshold (50,150)\", \"3x3 - Threshold (80,200)\"]])\n",
    "sobx_7x7_thres_lbs = np.asarray([[\"7x7 - Threshold (20,120)\", \"7x7 - Threshold (50,150)\", \"7x7 - Threshold (80,200)\"]])\n",
    "sobx_11x11_thres_lbs = np.asarray([[\"11x11 - Threshold (20,120)\", \"11x11 - Threshold (50,150)\", \"11x11 - Threshold (80,200)\"]])\n",
    "sobx_15x15_thres_lbs = np.asarray([[\"15x15 - Threshold (20,120)\", \"15x15 - Threshold (50,150)\", \"15x15 - Threshold (80,200)\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobx_thres = np.concatenate((sobx_3x3_thres, sobx_7x7_thres, sobx_11x11_thres, sobx_15x15_thres))\n",
    "sobx_thres_lbs = np.concatenate((sobx_3x3_thres_lbs, sobx_7x7_thres_lbs, sobx_11x11_thres_lbs, sobx_15x15_thres_lbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(sobx_thres, sobx_thres_lbs, \"Sobel (X Direction) Thresholds\", cols=3, show_ticks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the \"best\" results for Sobel in the X direction with thresholds values between in the interval _[20,120]_, using a kernel size of 15 (lines are very crisp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our best sobel results\n",
    "sobx_best = abs_sobel(undist_test_img_gray, kernel_size=15, thres=(20, 120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soby_3x3_thres = np.asarray([[abs_sobel(undist_test_img_gray, x_dir=False, thres=(20, 120)), abs_sobel(undist_test_img_gray, x_dir=False, thres=(50, 150)), abs_sobel(undist_test_img_gray, x_dir=False, thres=(80, 200))]])\n",
    "soby_7x7_thres = np.asarray([[abs_sobel(undist_test_img_gray, x_dir=False, kernel_size=7, thres=(20, 120)), abs_sobel(undist_test_img_gray, x_dir=False, kernel_size=7, thres=(50, 150)), abs_sobel(undist_test_img_gray, x_dir=False, kernel_size=7, thres=(80, 200))]])\n",
    "soby_11x11_thres = np.asarray([[abs_sobel(undist_test_img_gray, x_dir=False, kernel_size=11, thres=(20, 120)), abs_sobel(undist_test_img_gray, x_dir=False, kernel_size=11, thres=(50, 150)), abs_sobel(undist_test_img_gray, x_dir=False, kernel_size=11, thres=(80, 200))]])\n",
    "soby_15x15_thres = np.asarray([[abs_sobel(undist_test_img_gray, x_dir=False, kernel_size=15, thres=(20, 120)), abs_sobel(undist_test_img_gray, x_dir=False, kernel_size=15, thres=(50, 150)), abs_sobel(undist_test_img_gray, x_dir=False, kernel_size=15, thres=(80, 200))]])\n",
    "\n",
    "soby_3x3_thres_lbs = np.asarray([[\"3x3 - Threshold (20,120)\", \"3x3 - Threshold (50,150)\", \"3x3 - Threshold (80,200)\"]])\n",
    "soby_7x7_thres_lbs = np.asarray([[\"7x7 - Threshold (20,120)\", \"7x7 - Threshold (50,150)\", \"7x7 - Threshold (80,200)\"]])\n",
    "soby_11x11_thres_lbs = np.asarray([[\"11x11 - Threshold (20,120)\", \"11x11 - Threshold (50,150)\", \"11x11 - Threshold (80,200)\"]])\n",
    "soby_15x15_thres_lbs = np.asarray([[\"15x15 - Threshold (20,120)\", \"15x15 - Threshold (50,150)\", \"15x15 - Threshold (80,200)\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soby_thres = np.concatenate((soby_3x3_thres, soby_7x7_thres, soby_11x11_thres, soby_15x15_thres))\n",
    "soby_thres_lbs = np.concatenate((soby_3x3_thres_lbs, soby_7x7_thres_lbs, soby_11x11_thres_lbs, soby_15x15_thres_lbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(soby_thres, soby_thres_lbs, \"Sobel (Y Direction) Thresholds\", cols=3, show_ticks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Y direction, our best Sobel configuration is with thresholds in the interval _[20,120]_ and kernel size 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our best sobel y result\n",
    "soby_best = abs_sobel(undist_test_img_gray, x_dir=False, kernel_size=15, thres=(20, 120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sobel Magnitude in X and Y Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mag_sobel(gray_img, kernel_size=3, thres=(0, 255)):\n",
    "    \"\"\"\n",
    "    Computes sobel matrix in both x and y directions, merges them by computing the magnitude in both directions\n",
    "    and applies a threshold value to only set pixels within the specified range\n",
    "    \"\"\"\n",
    "    sx = cv2.Sobel(gray_img, cv2.CV_64F, 1, 0, ksize=kernel_size)\n",
    "    sy = cv2.Sobel(gray_img, cv2.CV_64F, 0, 1, ksize=kernel_size)\n",
    "    \n",
    "    sxy = np.sqrt(np.square(sx) + np.square(sy))\n",
    "    scaled_sxy = np.uint8(255 * sxy / np.max(sxy))\n",
    "    \n",
    "    sxy_binary = np.zeros_like(scaled_sxy)\n",
    "    sxy_binary[(scaled_sxy >= thres[0]) & (scaled_sxy <= thres[1])] = 1\n",
    "    \n",
    "    return sxy_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobxy_3x3_thres = np.asarray([[mag_sobel(undist_test_img_gray, kernel_size=3, thres=(20, 80)), mag_sobel(undist_test_img_gray, kernel_size=3, thres=(50, 150)), mag_sobel(undist_test_img_gray, kernel_size=3, thres=(80, 200))]])\n",
    "sobxy_7x7_thres = np.asarray([[mag_sobel(undist_test_img_gray, kernel_size=7, thres=(20, 80)), mag_sobel(undist_test_img_gray, kernel_size=7, thres=(50, 150)), mag_sobel(undist_test_img_gray, kernel_size=7, thres=(80, 200))]])\n",
    "sobxy_11x11_thres = np.asarray([[mag_sobel(undist_test_img_gray, kernel_size=11, thres=(20, 80)), mag_sobel(undist_test_img_gray, kernel_size=11, thres=(50, 150)), mag_sobel(undist_test_img_gray, kernel_size=11, thres=(80, 200))]])\n",
    "sobxy_15x15_thres = np.asarray([[mag_sobel(undist_test_img_gray, kernel_size=15, thres=(20, 80)), mag_sobel(undist_test_img_gray, kernel_size=15, thres=(50, 150)), mag_sobel(undist_test_img_gray, kernel_size=15, thres=(80, 200))]])\n",
    "\n",
    "sobxy_3x3_thres_lbs = np.asarray([[\"3x3 - Threshold (20,80)\", \"3x3 - Threshold (50,150)\", \"3x3 - Threshold (80,200)\"]])\n",
    "sobxy_7x7_thres_lbs = np.asarray([[\"7x7 - Threshold (20,80)\", \"7x7 - Threshold (50,150)\", \"7x7 - Threshold (80,200)\"]])\n",
    "sobxy_11x11_thres_lbs = np.asarray([[\"11x11 - Threshold (20,80)\", \"11x11 - Threshold (50,150)\", \"11x11 - Threshold (80,200)\"]])\n",
    "sobxy_15x15_thres_lbs = np.asarray([[\"15x15 - Threshold (20,80)\", \"15x15 - Threshold (50,150)\", \"15x15 - Threshold (80,200)\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobxy_thres = np.concatenate((sobxy_3x3_thres, sobxy_7x7_thres, sobxy_11x11_thres, sobxy_15x15_thres))\n",
    "sobxy_thres_lbs = np.concatenate((sobxy_3x3_thres_lbs, sobxy_7x7_thres_lbs, sobxy_11x11_thres_lbs, sobxy_15x15_thres_lbs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(sobxy_thres, sobxy_thres_lbs, \"Sobel (XY Magnitude) Thresholds\", cols=3, show_ticks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we plan to combine this Sobel result, we believe we get enough information for interval _[80, 200]_ and kernel size 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our best Sobel XY magnitude results\n",
    "sobxy_best = mag_sobel(undist_test_img_gray, kernel_size=15, thres=(80, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sobel With Gradient Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Sobel operation is extremely noisy and it becomes quite hard to decipher the result if we do not combine it with our previous sobel operations and produce a binary output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_sobel(gray_img, kernel_size=3, thres=(0, np.pi/2)):\n",
    "    \"\"\"\n",
    "    Computes sobel matrix in both x and y directions, gets their absolute values to find the direction of the gradient\n",
    "    and applies a threshold value to only set pixels within the specified range\n",
    "    \"\"\"\n",
    "    sx_abs = np.absolute(cv2.Sobel(gray_img, cv2.CV_64F, 1, 0, ksize=kernel_size))\n",
    "    sy_abs = np.absolute(cv2.Sobel(gray_img, cv2.CV_64F, 0, 1, ksize=kernel_size))\n",
    "    \n",
    "    dir_sxy = np.arctan2(sx_abs, sy_abs)\n",
    "\n",
    "    binary_output = np.zeros_like(dir_sxy)\n",
    "    binary_output[(dir_sxy >= thres[0]) & (dir_sxy <= thres[1])] = 1\n",
    "    \n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_sobels(sx_binary, sy_binary, sxy_magnitude_binary, gray_img, kernel_size=3, angle_thres=(0, np.pi/2)):\n",
    "    sxy_direction_binary = dir_sobel(gray_img, kernel_size=kernel_size, thres=angle_thres)\n",
    "    \n",
    "    combined = np.zeros_like(sxy_direction_binary)\n",
    "    # Sobel X returned the best output so we keep all of its results. We perform a binary and on all the other sobels    \n",
    "    combined[(sx_binary == 1) | ((sy_binary == 1) & (sxy_magnitude_binary == 1) & (sxy_direction_binary == 1))] = 1\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = combined_sobels(sobx_best, soby_best, sobxy_best, undist_test_img_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobxy_combined_dir_3x3_thres = np.asarray([[combined_sobels(sobx_best, soby_best, sobxy_best, undist_test_img_gray, kernel_size=3, angle_thres=(0, np.pi/4)),\n",
    "                                            combined_sobels(sobx_best, soby_best, sobxy_best, undist_test_img_gray, kernel_size=3, angle_thres=(np.pi/4, np.pi/2)),\n",
    "                                            combined_sobels(sobx_best, soby_best, sobxy_best, undist_test_img_gray, kernel_size=3, angle_thres=(np.pi/3, np.pi/2))\n",
    "                                           ]])\n",
    "\n",
    "sobxy_combined_dir_7x7_thres = np.asarray([[combined_sobels(sobx_best, soby_best, sobxy_best, undist_test_img_gray, kernel_size=7, angle_thres=(0, np.pi/4)),\n",
    "                                            combined_sobels(sobx_best, soby_best, sobxy_best, undist_test_img_gray, kernel_size=7, angle_thres=(np.pi/4, np.pi/2)),\n",
    "                                            combined_sobels(sobx_best, soby_best, sobxy_best, undist_test_img_gray, kernel_size=7, angle_thres=(np.pi/3, np.pi/2))\n",
    "                                           ]])\n",
    "\n",
    "sobxy_combined_dir_11x11_thres = np.asarray([[combined_sobels(sobx_best, soby_best, sobxy_best, undist_test_img_gray, kernel_size=11, angle_thres=(0, np.pi/4)),\n",
    "                                            combined_sobels(sobx_best, soby_best, sobxy_best, undist_test_img_gray, kernel_size=11, angle_thres=(np.pi/4, np.pi/2)),\n",
    "                                            combined_sobels(sobx_best, soby_best, sobxy_best, undist_test_img_gray, kernel_size=11, angle_thres=(np.pi/3, np.pi/2))\n",
    "                                           ]])\n",
    "\n",
    "sobxy_combined_dir_15x15_thres = np.asarray([[combined_sobels(sobx_best, soby_best, sobxy_best, undist_test_img_gray, kernel_size=15, angle_thres=(0, np.pi/4)),\n",
    "                                            combined_sobels(sobx_best, soby_best, sobxy_best, undist_test_img_gray, kernel_size=15, angle_thres=(np.pi/4, np.pi/2)),\n",
    "                                            combined_sobels(sobx_best, soby_best, sobxy_best, undist_test_img_gray, kernel_size=15, angle_thres=(np.pi/3, np.pi/2))\n",
    "                                           ]])\n",
    "\n",
    "\n",
    "sobxy_combined_dir_3x3_thres_lbs = np.asarray([[\"3x3 - Combined (0, pi/4)\", \"3x3 - Combined (pi/4, pi/2)\", \"3x3 - Combined (pi/3, pi/2)\"]])\n",
    "sobxy_combined_dir_7x7_thres_lbs = np.asarray([[\"7x7 - Combined (0, pi/4)\", \"7x7 - Combined (pi/4, pi/2)\", \"7x7 - Combined (pi/3, pi/2)\"]])\n",
    "sobxy_combined_dir_11x11_thres_lbs = np.asarray([[\"11x11 - Combined (0, pi/4)\", \"11x11 - Combined (pi/4, pi/2)\", \"11x11 - Combined (pi/3, pi/2)\"]])\n",
    "sobxy_combined_dir_15x15_thres_lbs = np.asarray([[\"15x15 - Combined (0, pi/4)\", \"15x15 - Combined (pi/4, pi/2)\", \"15x15 - Combined (pi/3, pi/2)\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobxy_combined_dir_thres = np.concatenate((sobxy_combined_dir_3x3_thres, sobxy_combined_dir_7x7_thres, sobxy_combined_dir_11x11_thres, sobxy_combined_dir_15x15_thres))\n",
    "sobxy_combined_dir_thres_lbs = np.concatenate((sobxy_combined_dir_3x3_thres_lbs, sobxy_combined_dir_7x7_thres_lbs, sobxy_combined_dir_11x11_thres_lbs, sobxy_combined_dir_15x15_thres_lbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(sobxy_combined_dir_thres, sobxy_combined_dir_thres_lbs, \"Combined With Gradient Direction\", cols=3, show_ticks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the kernel size does not seem to affect the direction sobel operation so much. It's also hard to determine which one is the \"best\" result out of all our visualisations. Since we want to retain only information about lane lines, we are inclined to elect interval _[pi/4, pi/2]_ as our best configuration as it gives us the most room for manoeuvre. Kernel size of 15x15 produces the least noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_combined_best = combined_sobels(sobx_best, soby_best, sobxy_best, undist_test_img_gray, kernel_size=15, angle_thres=(np.pi/4, np.pi/2))                                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting image from combined variant of sobel operations shows promise. We should next combine it with the results we obtained from our HLS' color thresholding method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Color And Gradient (Sobel) Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_binary = np.dstack((np.zeros_like(sobel_combined_best), sobel_combined_best, undistorted_yellow_white_hls_img_bin)) * 255\n",
    "color_binary = color_binary.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_binary = np.zeros_like(undistorted_yellow_white_hls_img_bin)\n",
    "combined_binary[(sobel_combined_best == 1) | (undistorted_yellow_white_hls_img_bin == 1)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_binaries = [[color_binary, combined_binary]]\n",
    "combined_binaries_lbs = np.asarray([[\"Stacked Thresholds\", \"Combined Color And Gradient Thresholds\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(combined_binaries, combined_binaries_lbs, \"Color And Binary Combined Gradient And HLS (S) Thresholss\", cols=2, fig_size=(17, 6), show_ticks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very encouraging and it seems we have found the right parameters to detect lanes in a robust manner. We turn next to applying a perspective transform to our image and produce a _bird's eye view_ of the lane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define a trapezoidal region in the 2D image that will go through a perspective transform to convert into a bird's eye view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_combined = np.copy(undist_test_imgs[1])\n",
    "(bottom_px, right_px) = (copy_combined.shape[0] - 1, copy_combined.shape[1] - 1) \n",
    "pts = np.array([[210,bottom_px],[595,450],[690,450], [1110, bottom_px]], np.int32)\n",
    "cv2.polylines(copy_combined,[pts],True,(255,0,0), 10)\n",
    "plt.axis('off')\n",
    "plt.imshow(copy_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perspective_transform_matrices(src, dst):\n",
    "    \"\"\"\n",
    "    Returns the tuple (M, M_inv) where M represents the matrix to use for perspective transform\n",
    "    and M_inv is the matrix used to revert the transformed image back to the original one\n",
    "    \"\"\"\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    M_inv = cv2.getPerspectiveTransform(dst, src)\n",
    "    \n",
    "    return (M, M_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perspective_transform(img, src, dst):   \n",
    "    \"\"\"\n",
    "    Applies a perspective \n",
    "    \"\"\"\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pts = pts.astype(np.float32)\n",
    "dst_pts = np.array([[200, bottom_px], [200, 0], [1000, 0], [1000, bottom_px]], np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_persp_tr = perspective_transform(undistorted_test_img, src_pts, dst_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_img_persp_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the lane is curved, and our perspective transform takes this into account too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_pers_tr = np.asarray(list(map(lambda img: perspective_transform(img, src_pts, dst_pts), undist_test_imgs))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_persp_img = np.copy(test_imgs_pers_tr[1])\n",
    "dst = dst_pts.astype(np.int32)\n",
    "cv2.polylines(test_persp_img,[dst],True,(255,0,0), 10)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,10))\n",
    "ax[0].imshow(test_imgs_pers_tr[5])\n",
    "ax[0].set_title(\"Perspecting Transform - Curved Lines\")\n",
    "\n",
    "ax[1].imshow(test_persp_img)\n",
    "ax[1].set_title(\"Perspective Transform - Straight Lines\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our perspective transform keeps straight lines straight, which is a required sanity check. The curved lines however are not perfect on the example above, but they should not cause unsurmountable problems for our algorithm either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_undist_imgs_and_p_tr = np.asarray(list(zip(undist_test_imgs, test_imgs_pers_tr)))\n",
    "test_undist_imgs_and_p_tr_names = np.asarray(list(zip(undist_test_img_names, undist_test_img_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(test_undist_imgs_and_p_tr, test_undist_imgs_and_p_tr_names, \"Undistorted and Birds View Image\", fig_size=(15, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage it is fitting to start definining a pipeline of operations to perform and visualise perspective transform on thresholding binary images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_binary_thresholded_img(undist_img):\n",
    "    \"\"\"\n",
    "    Applies a combination of binary Sobel and color thresholding to an undistorted image\n",
    "    Those binary images are then combined to produce the returned binary image\n",
    "    \"\"\"\n",
    "    undist_img_gray = to_lab(undist_img)[:,:,0]\n",
    "    sx = abs_sobel(undist_img_gray, kernel_size=15, thres=(20, 120))\n",
    "    sy = abs_sobel(undist_img_gray, x_dir=False, kernel_size=15, thres=(20, 120))\n",
    "    sxy = mag_sobel(undist_img_gray, kernel_size=15, thres=(80, 200))\n",
    "    sxy_combined_dir = combined_sobels(sx, sy, sxy, undist_img_gray, kernel_size=15, angle_thres=(np.pi/4, np.pi/2))   \n",
    "    \n",
    "    hls_w_y_thres = compute_hls_white_yellow_binary(undist_img)\n",
    "    \n",
    "    combined_binary = np.zeros_like(hls_w_y_thres)\n",
    "    combined_binary[(sxy_combined_dir == 1) | (hls_w_y_thres == 1)] = 1\n",
    "        \n",
    "    return combined_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_combined_binary_thres = np.asarray(list(map(lambda img: get_combined_binary_thresholded_img(img), undist_test_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_psp_tr = np.asarray(list(map(lambda img: perspective_transform(img, src_pts, dst_pts), undist_test_imgs))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_combined_binary_psp_tr = np.asarray(list(map(lambda img: perspective_transform(img, src_pts, dst_pts), test_imgs_combined_binary_thres))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_combined_binary_and_psp_tr = np.asarray(list(zip(test_imgs_psp_tr[:,:,:,0],test_imgs_combined_binary_thres, test_imgs_combined_binary_psp_tr)))\n",
    "test_imgs_combined_binary_and_psp_tr_names = np.asarray(list(zip(undist_test_img_names,undist_test_img_names, undist_test_img_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(test_imgs_combined_binary_and_psp_tr, test_imgs_combined_binary_and_psp_tr_names, \"Combined Binary And Perspective Transform Images\", cols=3, fig_size=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perspective transform, then application of color and gradient thresholding enable us to clearly identify the position of the lanes on the bird's eye view image. However, we also notice some small dots and other visual artifacts. We must build a robust lane finder in order not to be caught by these glitches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_example = test_imgs_combined_binary_and_psp_tr[6][2]\n",
    "histogram = np.sum(img_example[img_example.shape[0]//2:,:], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,4))\n",
    "ax[0].imshow(img_example, cmap='gray')\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].set_title(\"Binary Thresholded Perspective Transform Image\")\n",
    "\n",
    "ax[1].plot(histogram)\n",
    "ax[1].set_title(\"Histogram Of Pixel Intensities (Image Bottom Half)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Definition\n",
    "\n",
    "Let's now define our full pipeline for lane detection on video frames. We will use Python classes to encapsulate a information and use it across frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def create_queue(length = 10):\n",
    "    return deque(maxlen=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneLine:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.polynomial_coeff = None\n",
    "        self.line_fit_x = None\n",
    "        self.non_zero_x = []\n",
    "        self.non_zero_y = []\n",
    "        self.windows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LaneLineHistory:\n",
    "    def __init__(self, queue_depth=2, test_points=[50, 300, 500, 700], poly_max_deviation_distance=150):\n",
    "        self.lane_lines = create_queue(queue_depth)\n",
    "        self.smoothed_poly = None\n",
    "        self.test_points = test_points\n",
    "        self.poly_max_deviation_distance = poly_max_deviation_distance\n",
    "    \n",
    "    def append(self, lane_line, force=False):\n",
    "        if len(self.lane_lines) == 0 or force:\n",
    "            self.lane_lines.append(lane_line)\n",
    "            self.get_smoothed_polynomial()\n",
    "            return True\n",
    "        \n",
    "        test_y_smooth = np.asarray(list(map(lambda x: self.smoothed_poly[0] * x**2 + self.smoothed_poly[1] * x + self.smoothed_poly[2], self.test_points)))\n",
    "        test_y_new = np.asarray(list(map(lambda x: lane_line.polynomial_coeff[0] * x**2 + lane_line.polynomial_coeff[1] * x + lane_line.polynomial_coeff[2], self.test_points)))\n",
    "        \n",
    "        dist = np.absolute(test_y_smooth - test_y_new)\n",
    "        \n",
    "        #dist = np.absolute(self.smoothed_poly - lane_line.polynomial_coeff)\n",
    "        #dist_max = np.absolute(self.smoothed_poly * self.poly_max_deviation_distance)\n",
    "        max_dist = dist[np.argmax(dist)]\n",
    "        \n",
    "        if max_dist > self.poly_max_deviation_distance:\n",
    "            print(\"**** MAX DISTANCE BREACHED ****\")\n",
    "            print(\"y_smooth={0} - y_new={1} - distance={2} - max-distance={3}\".format(test_y_smooth, test_y_new, max_dist, self.poly_max_deviation_distance))\n",
    "            return False\n",
    "        \n",
    "        self.lane_lines.append(lane_line)\n",
    "        self.get_smoothed_polynomial()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_smoothed_polynomial(self):\n",
    "        all_coeffs = np.asarray(list(map(lambda lane_line: lane_line.polynomial_coeff, self.lane_lines)))\n",
    "        self.smoothed_poly = np.mean(all_coeffs, axis=0)\n",
    "        \n",
    "        return self.smoothed_poly\n",
    "                                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedLaneDetectorWithMemory:\n",
    "    \"\"\"\n",
    "    The AdvancedLaneDetectorWithMemory is a class that can detect lines on the road\n",
    "    \"\"\"\n",
    "    def __init__(self, objpts, imgpts, psp_src, psp_dst, sliding_windows_per_line, \n",
    "                 sliding_window_half_width, sliding_window_recenter_thres, \n",
    "                 small_img_size=(256, 144), small_img_x_offset=20, small_img_y_offset=10,\n",
    "                 img_dimensions=(720, 1280), lane_width_px=800, \n",
    "                 lane_center_px_psp=600, real_world_lane_size_meters=(32, 3.7)):\n",
    "        self.objpts = objpts\n",
    "        self.imgpts = imgpts\n",
    "        (self.M_psp, self.M_inv_psp) = compute_perspective_transform_matrices(psp_src, psp_dst)\n",
    "\n",
    "        self.sliding_windows_per_line = sliding_windows_per_line\n",
    "        self.sliding_window_half_width = sliding_window_half_width\n",
    "        self.sliding_window_recenter_thres = sliding_window_recenter_thres\n",
    "        \n",
    "        self.small_img_size = small_img_size\n",
    "        self.small_img_x_offset = small_img_x_offset\n",
    "        self.small_img_y_offset = small_img_y_offset\n",
    "        \n",
    "        self.img_dimensions = img_dimensions\n",
    "        self.lane_width_px = lane_width_px\n",
    "        self.lane_center_px_psp = lane_center_px_psp \n",
    "        self.real_world_lane_size_meters = real_world_lane_size_meters\n",
    "\n",
    "        # We can pre-compute some data here\n",
    "        self.ym_per_px = self.real_world_lane_size_meters[0] / self.img_dimensions[0]\n",
    "        self.xm_per_px = self.real_world_lane_size_meters[1] / self.lane_width_px\n",
    "        self.ploty = np.linspace(0, self.img_dimensions[0] - 1, self.img_dimensions[0])\n",
    "        \n",
    "        self.previous_left_lane_line = None\n",
    "        self.previous_right_lane_line = None\n",
    "        \n",
    "        self.previous_left_lane_lines = LaneLineHistory()\n",
    "        self.previous_right_lane_lines = LaneLineHistory()\n",
    "        \n",
    "        self.total_img_count = 0\n",
    "        \n",
    "    \n",
    "    def process_image(self, img):\n",
    "        \"\"\"\n",
    "        Attempts to find lane lines on the given image and returns an image with lane area colored in green\n",
    "        as well as small intermediate images overlaid on top to understand how the algorithm is performing\n",
    "        \"\"\"\n",
    "        # First step - undistort the image using the instance's object and image points\n",
    "        undist_img = undistort_image(img, self.objpts, self.imgpts)\n",
    "        \n",
    "        # Produce binary thresholded image from color and gradients\n",
    "        thres_img = get_combined_binary_thresholded_img(undist_img)\n",
    "        \n",
    "        # Create the undistorted and binary perspective transforms\n",
    "        img_size = (undist_img.shape[1], undist_img.shape[0])\n",
    "        undist_img_psp = cv2.warpPerspective(undist_img, self.M_psp, img_size, flags=cv2.INTER_LINEAR)\n",
    "        thres_img_psp = cv2.warpPerspective(thres_img, self.M_psp, img_size, flags=cv2.INTER_LINEAR)\n",
    "        \n",
    "        ll, rl = self.compute_lane_lines(thres_img_psp)\n",
    "        lcr, rcr, lco = self.compute_lane_curvature(ll, rl)\n",
    "\n",
    "        drawn_lines = self.draw_lane_lines(thres_img_psp, ll, rl)        \n",
    "        #plt.imshow(drawn_lines)\n",
    "        \n",
    "        drawn_lines_regions = self.draw_lane_lines_regions(thres_img_psp, ll, rl)\n",
    "        #plt.imshow(drawn_lines_regions)\n",
    "        \n",
    "        drawn_lane_area = self.draw_lane_area(thres_img_psp, undist_img, ll, rl)        \n",
    "        #plt.imshow(drawn_lane_area)\n",
    "        \n",
    "        drawn_hotspots = self.draw_lines_hotspots(thres_img_psp, ll, rl)\n",
    "        \n",
    "        combined_lane_img = self.combine_images(drawn_lane_area, drawn_lines, drawn_lines_regions, drawn_hotspots, undist_img_psp)\n",
    "        final_img = self.draw_lane_curvature_text(combined_lane_img, lcr, rcr, lco)\n",
    "        \n",
    "        self.total_img_count += 1\n",
    "        self.previous_left_lane_line = ll\n",
    "        self.previous_right_lane_line = rl\n",
    "        \n",
    "        return final_img\n",
    "    \n",
    "    def draw_lane_curvature_text(self, img, left_curvature_meters, right_curvature_meters, center_offset_meters):\n",
    "        \"\"\"\n",
    "        Returns an image with curvature information inscribed\n",
    "        \"\"\"\n",
    "        \n",
    "        offset_y = self.small_img_size[1] * 1 + self.small_img_y_offset * 5\n",
    "        offset_x = self.small_img_x_offset\n",
    "        \n",
    "        template = \"{0:17}{1:17}{2:17}\"\n",
    "        txt_header = template.format(\"Left Curvature\", \"Right Curvature\", \"Center Alignment\") \n",
    "        print(txt_header)\n",
    "        txt_values = template.format(\"{:.4f}m\".format(left_curvature_meters), \n",
    "                                     \"{:.4f}m\".format(right_curvature_meters),\n",
    "                                     \"{:.4f}m Right\".format(center_offset_meters))\n",
    "        if center_offset_meters < 0.0:\n",
    "            txt_values = template.format(\"{:.4f}m\".format(left_curvature_meters), \n",
    "                                     \"{:.4f}m\".format(right_curvature_meters),\n",
    "                                     \"{:.4f}m Left\".format(math.fabs(center_offset_meters)))\n",
    "            \n",
    "        \n",
    "        print(txt_values)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(img, txt_header, (offset_x, offset_y), font, 1, (255,255,255), 1, cv2.LINE_AA)\n",
    "        cv2.putText(img, txt_values, (offset_x, offset_y + self.small_img_y_offset * 5), font, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def combine_images(self, lane_area_img, lines_img, lines_regions_img, lane_hotspots_img, psp_color_img):        \n",
    "        \"\"\"\n",
    "        Returns a new image made up of the lane area image, and the remaining lane images are overlaid as\n",
    "        small images in a row at the top of the the new image\n",
    "        \"\"\"\n",
    "        small_lines = cv2.resize(lines_img, self.small_img_size)\n",
    "        small_region = cv2.resize(lines_regions_img, self.small_img_size)\n",
    "        small_hotspots = cv2.resize(lane_hotspots_img, self.small_img_size)\n",
    "        small_color_psp = cv2.resize(psp_color_img, self.small_img_size)\n",
    "                \n",
    "        lane_area_img[self.small_img_y_offset: self.small_img_y_offset + self.small_img_size[1], self.small_img_x_offset: self.small_img_x_offset + self.small_img_size[0]] = small_lines\n",
    "        \n",
    "        start_offset_y = self.small_img_y_offset \n",
    "        start_offset_x = 2 * self.small_img_x_offset + self.small_img_size[0]\n",
    "        lane_area_img[start_offset_y: start_offset_y + self.small_img_size[1], start_offset_x: start_offset_x + self.small_img_size[0]] = small_region\n",
    "        \n",
    "        start_offset_y = self.small_img_y_offset \n",
    "        start_offset_x = 3 * self.small_img_x_offset + 2 * self.small_img_size[0]\n",
    "        lane_area_img[start_offset_y: start_offset_y + self.small_img_size[1], start_offset_x: start_offset_x + self.small_img_size[0]] = small_hotspots\n",
    "\n",
    "        start_offset_y = self.small_img_y_offset \n",
    "        start_offset_x = 4 * self.small_img_x_offset + 3 * self.small_img_size[0]\n",
    "        lane_area_img[start_offset_y: start_offset_y + self.small_img_size[1], start_offset_x: start_offset_x + self.small_img_size[0]] = small_color_psp\n",
    "        \n",
    "        \n",
    "        return lane_area_img\n",
    "    \n",
    "        \n",
    "    def draw_lane_area(self, warped_img, undist_img, left_line, right_line):\n",
    "        \"\"\"\n",
    "        Returns an image where the inside of the lane has been colored in bright green\n",
    "        \"\"\"\n",
    "        # Create an image to draw the lines on\n",
    "        warp_zero = np.zeros_like(warped_img).astype(np.uint8)\n",
    "        color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "\n",
    "        ploty = np.linspace(0, warped_img.shape[0] - 1, warped_img.shape[0])\n",
    "        # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "        pts_left = np.array([np.transpose(np.vstack([left_line.line_fit_x, ploty]))])\n",
    "        pts_right = np.array([np.flipud(np.transpose(np.vstack([right_line.line_fit_x, ploty])))])\n",
    "        pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "        # Draw the lane onto the warped blank image\n",
    "        cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "        # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "        newwarp = cv2.warpPerspective(color_warp, self.M_inv_psp, (undist_img.shape[1], undist_img.shape[0])) \n",
    "        # Combine the result with the original image\n",
    "        result = cv2.addWeighted(undist_img, 1, newwarp, 0.3, 0)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "        \n",
    "    def draw_lane_lines(self, warped_img, left_line, right_line):\n",
    "        \"\"\"\n",
    "        Returns an image where the computed lane lines have been drawn on top of the original warped binary image\n",
    "        \"\"\"\n",
    "        # Create an output image with 3 colors (RGB) from the binary warped image to draw on and  visualize the result\n",
    "        out_img = np.dstack((warped_img, warped_img, warped_img))*255\n",
    "        \n",
    "        # Now draw the lines\n",
    "        ploty = np.linspace(0, warped_img.shape[0] - 1, warped_img.shape[0])\n",
    "        pts_left = np.dstack((left_line.line_fit_x, ploty)).astype(np.int32)\n",
    "        pts_right = np.dstack((right_line.line_fit_x, ploty)).astype(np.int32)\n",
    "\n",
    "        cv2.polylines(out_img, pts_left, False,  (255, 140,0), 5)\n",
    "        cv2.polylines(out_img, pts_right, False, (255, 140,0), 5)\n",
    "        \n",
    "        for low_pt, high_pt in left_line.windows:\n",
    "            cv2.rectangle(out_img, low_pt, high_pt, (0, 255, 0), 3)\n",
    "\n",
    "        for low_pt, high_pt in right_line.windows:            \n",
    "            cv2.rectangle(out_img, low_pt, high_pt, (0, 255, 0), 3)           \n",
    "        \n",
    "        return out_img    \n",
    "    \n",
    "    def draw_lane_lines_regions(self, warped_img, left_line, right_line):\n",
    "        \"\"\"\n",
    "        Returns an image where the computed left and right lane areas have been drawn on top of the original warped binary image\n",
    "        \"\"\"\n",
    "        # Generate a polygon to illustrate the search window area\n",
    "        # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "        margin = self.sliding_window_half_width\n",
    "        ploty = np.linspace(0, warped_img.shape[0] - 1, warped_img.shape[0])\n",
    "        \n",
    "        left_line_window1 = np.array([np.transpose(np.vstack([left_line.line_fit_x - margin, ploty]))])\n",
    "        left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_line.line_fit_x + margin, \n",
    "                                      ploty])))])\n",
    "        left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "        \n",
    "        right_line_window1 = np.array([np.transpose(np.vstack([right_line.line_fit_x - margin, ploty]))])\n",
    "        right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_line.line_fit_x + margin, \n",
    "                                      ploty])))])\n",
    "        right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "        # Create RGB image from binary warped image\n",
    "        region_img = np.dstack((warped_img, warped_img, warped_img)) * 255\n",
    "\n",
    "        # Draw the lane onto the warped blank image\n",
    "        cv2.fillPoly(region_img, np.int_([left_line_pts]), (0, 255, 0))\n",
    "        cv2.fillPoly(region_img, np.int_([right_line_pts]), (0, 255, 0))\n",
    "        \n",
    "        return region_img\n",
    "\n",
    "\n",
    "    def draw_lines_hotspots(self, warped_img, left_line, right_line):\n",
    "        \"\"\"\n",
    "        Returns a RGB image where the portions of the lane lines that were\n",
    "        identified by our pipeline are colored in yellow (left) and blue (right)\n",
    "        \"\"\"\n",
    "        out_img = np.dstack((warped_img, warped_img, warped_img))*255\n",
    "        \n",
    "        out_img[left_line.non_zero_y, left_line.non_zero_x] = [255, 255, 0]\n",
    "        out_img[right_line.non_zero_y, right_line.non_zero_x] = [0, 0, 255]\n",
    "        \n",
    "        return out_img\n",
    "\n",
    "    def compute_lane_curvature(self, left_line, right_line):\n",
    "        \"\"\"\n",
    "        Returns the triple (left_curvature, right_curvature, lane_center_offset), which are all in meters\n",
    "        \"\"\"        \n",
    "        ploty = self.ploty\n",
    "        y_eval = np.max(ploty)\n",
    "        # Define conversions in x and y from pixels space to meters\n",
    "        \n",
    "        leftx = left_line.line_fit_x\n",
    "        rightx = right_line.line_fit_x\n",
    "        \n",
    "        # Fit new polynomials: find x for y in real-world space\n",
    "        left_fit_cr = np.polyfit(ploty * self.ym_per_px, leftx * self.xm_per_px, 2)\n",
    "        right_fit_cr = np.polyfit(ploty * self.ym_per_px, rightx * self.xm_per_px, 2)\n",
    "        \n",
    "        # Now calculate the radii of the curvature\n",
    "        left_curverad = ((1 + (2 * left_fit_cr[0] * y_eval * self.ym_per_px + left_fit_cr[1])**2)**1.5) / np.absolute(2 * left_fit_cr[0])\n",
    "        right_curverad = ((1 + (2 *right_fit_cr[0] * y_eval * self.ym_per_px + right_fit_cr[1])**2)**1.5) / np.absolute(2 * right_fit_cr[0])\n",
    "        \n",
    "        # Use our computed polynomial to determine the car's center position in image space, then\n",
    "        left_fit = left_line.polynomial_coeff\n",
    "        right_fit = right_line.polynomial_coeff\n",
    "        \n",
    "        center_offset_img_space = (((left_fit[0] * y_eval**2 + left_fit[1] * y_eval + left_fit[2]) + \n",
    "                   (right_fit[0] * y_eval**2 + right_fit[1] * y_eval + right_fit[2])) / 2) - self.lane_center_px_psp\n",
    "        center_offset_real_world_m = center_offset_img_space * self.xm_per_px\n",
    "        \n",
    "        # Now our radius of curvature is in meters        \n",
    "        return left_curverad, right_curverad, center_offset_real_world_m\n",
    "        \n",
    "        \n",
    "        \n",
    "    def compute_lane_lines(self, warped_img):\n",
    "        \"\"\"\n",
    "        Returns the tuple (left_lane_line, right_lane_line) which represents respectively the LaneLine instances for\n",
    "        the computed left and right lanes, for the supplied binary warped image\n",
    "        \"\"\"\n",
    "\n",
    "        # Take a histogram of the bottom half of the image, summing pixel values column wise \n",
    "        histogram = np.sum(warped_img[warped_img.shape[0]//2:,:], axis=0)\n",
    "        \n",
    "        # Find the peak of the left and right halves of the histogram\n",
    "        # These will be the starting point for the left and right lines \n",
    "        midpoint = np.int(histogram.shape[0]//2)\n",
    "        leftx_base = np.argmax(histogram[:midpoint])\n",
    "        rightx_base = np.argmax(histogram[midpoint:]) + midpoint # don't forget to offset by midpoint!\n",
    "        \n",
    "\n",
    "        # Set height of windows\n",
    "        window_height = np.int(warped_img.shape[0]//self.sliding_windows_per_line)\n",
    "        # Identify the x and y positions of all nonzero pixels in the image\n",
    "        # NOTE: nonzero returns a tuple of arrays in y and x directions\n",
    "        nonzero = warped_img.nonzero()\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        \n",
    "        total_non_zeros = len(nonzeroy)\n",
    "        non_zero_found_pct = 0.0\n",
    "        \n",
    "        # Current positions to be updated for each window\n",
    "        leftx_current = leftx_base\n",
    "        rightx_current = rightx_base    \n",
    "\n",
    "\n",
    "        # Set the width of the windows +/- margin\n",
    "        margin = self.sliding_window_half_width\n",
    "        # Set minimum number of pixels found to recenter window\n",
    "        minpix = self.sliding_window_recenter_thres\n",
    "        # Create empty lists to receive left and right lane pixel indices\n",
    "        left_lane_inds = []\n",
    "        right_lane_inds = []\n",
    "        \n",
    "        # Our lane line objects we store the result of this computation\n",
    "        left_line = LaneLine()\n",
    "        right_line = LaneLine()\n",
    "                        \n",
    "        if self.previous_left_lane_line is not None and self.previous_right_lane_line is not None:\n",
    "            # We have already computed the lane lines polynomials from a previous image\n",
    "            left_lane_inds = ((nonzerox > (self.previous_left_lane_line.polynomial_coeff[0] * (nonzeroy**2) \n",
    "                                           + self.previous_left_lane_line.polynomial_coeff[1] * nonzeroy \n",
    "                                           + self.previous_left_lane_line.polynomial_coeff[2] - margin)) \n",
    "                              & (nonzerox < (self.previous_left_lane_line.polynomial_coeff[0] * (nonzeroy**2) \n",
    "                                            + self.previous_left_lane_line.polynomial_coeff[1] * nonzeroy \n",
    "                                            + self.previous_left_lane_line.polynomial_coeff[2] + margin))) \n",
    "\n",
    "            right_lane_inds = ((nonzerox > (self.previous_right_lane_line.polynomial_coeff[0] * (nonzeroy**2) \n",
    "                                           + self.previous_right_lane_line.polynomial_coeff[1] * nonzeroy \n",
    "                                           + self.previous_right_lane_line.polynomial_coeff[2] - margin)) \n",
    "                              & (nonzerox < (self.previous_right_lane_line.polynomial_coeff[0] * (nonzeroy**2) \n",
    "                                            + self.previous_right_lane_line.polynomial_coeff[1] * nonzeroy \n",
    "                                            + self.previous_right_lane_line.polynomial_coeff[2] + margin))) \n",
    "            \n",
    "            non_zero_found_left = np.sum(left_lane_inds)\n",
    "            non_zero_found_right = np.sum(right_lane_inds)\n",
    "            non_zero_found_pct = (non_zero_found_left + non_zero_found_right) / total_non_zeros\n",
    "           \n",
    "            print(\"[Previous lane] Found pct={0}\".format(non_zero_found_pct))\n",
    "            #print(left_lane_inds)\n",
    "        \n",
    "        if non_zero_found_pct < 0.85:\n",
    "            print(\"Non zeros found below thresholds, begining sliding window - pct={0}\".format(non_zero_found_pct))\n",
    "            left_lane_inds = []\n",
    "            right_lane_inds = []\n",
    "\n",
    "            # Step through the windows one by one\n",
    "            for window in range(self.sliding_windows_per_line):\n",
    "                # Identify window boundaries in x and y (and right and left)\n",
    "                # We are moving our windows from the bottom to the top of the screen (highest to lowest y value)\n",
    "                win_y_low = warped_img.shape[0] - (window + 1)* window_height\n",
    "                win_y_high = warped_img.shape[0] - window * window_height\n",
    "\n",
    "                # Defining our window's coverage in the horizontal (i.e. x) direction \n",
    "                # Notice that the window's width is twice the margin\n",
    "                win_xleft_low = leftx_current - margin\n",
    "                win_xleft_high = leftx_current + margin\n",
    "                win_xright_low = rightx_current - margin\n",
    "                win_xright_high = rightx_current + margin\n",
    "\n",
    "                left_line.windows.append([(win_xleft_low,win_y_low),(win_xleft_high,win_y_high)])\n",
    "                right_line.windows.append([(win_xright_low,win_y_low),(win_xright_high,win_y_high)])\n",
    "\n",
    "                # Super crytic and hard to understand...\n",
    "                # Basically nonzerox and nonzeroy have the same size and any nonzero pixel is identified by\n",
    "                # (nonzeroy[i],nonzerox[i]), therefore we just return the i indices within the window that are nonzero\n",
    "                # and can then index into nonzeroy and nonzerox to find the ACTUAL pixel coordinates that are not zero\n",
    "                good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "                (nonzerox >= win_xleft_low) &  (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "                good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "                (nonzerox >= win_xright_low) &  (nonzerox < win_xright_high)).nonzero()[0]\n",
    "                            \n",
    "                # Append these indices to the lists\n",
    "                left_lane_inds.append(good_left_inds)\n",
    "                right_lane_inds.append(good_right_inds)\n",
    "\n",
    "                # If you found > minpix pixels, recenter next window on their mean position\n",
    "                if len(good_left_inds) > minpix:\n",
    "                    leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "                if len(good_right_inds) > minpix:        \n",
    "                    rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "            # Concatenate the arrays of indices since we now have a list of multiple arrays (e.g. ([1,3,6],[8,5,2]))\n",
    "            # We want to create a single array with elements from all those lists (e.g. [1,3,6,8,5,2])\n",
    "            # These are the indices that are non zero in our sliding windows\n",
    "            left_lane_inds = np.concatenate(left_lane_inds)\n",
    "            right_lane_inds = np.concatenate(right_lane_inds)\n",
    "            \n",
    "            non_zero_found_left = np.sum(left_lane_inds)\n",
    "            non_zero_found_right = np.sum(right_lane_inds)\n",
    "            non_zero_found_pct = (non_zero_found_left + non_zero_found_right) / total_non_zeros\n",
    "           \n",
    "            print(\"[Sliding windows] Found pct={0}\".format(non_zero_found_pct))\n",
    "            \n",
    "    \n",
    "        # Extract left and right line pixel positions\n",
    "        leftx = nonzerox[left_lane_inds]\n",
    "        lefty = nonzeroy[left_lane_inds] \n",
    "        rightx = nonzerox[right_lane_inds]\n",
    "        righty = nonzeroy[right_lane_inds] \n",
    "        \n",
    "        #print(\"[LEFT] Number of hot pixels={0}\".format(len(leftx)))\n",
    "        #print(\"[RIGHT] Number of hot pixels={0}\".format(len(rightx)))\n",
    "        # Fit a second order polynomial to each\n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "        #print(\"Poly left {0}\".format(left_fit))\n",
    "        #print(\"Poly right {0}\".format(right_fit))\n",
    "        left_line.polynomial_coeff = left_fit\n",
    "        right_line.polynomial_coeff = right_fit\n",
    "        \n",
    "        if not self.previous_left_lane_lines.append(left_line):\n",
    "            left_fit = self.previous_left_lane_lines.get_smoothed_polynomial()\n",
    "            left_line.polynomial_coeff = left_fit\n",
    "            self.previous_left_lane_lines.append(left_line, force=True)\n",
    "            print(\"**** REVISED Poly left {0}\".format(left_fit))            \n",
    "        #else:\n",
    "            #left_fit = self.previous_left_lane_lines.get_smoothed_polynomial()\n",
    "            #left_line.polynomial_coeff = left_fit\n",
    "\n",
    "\n",
    "        if not self.previous_right_lane_lines.append(right_line):\n",
    "            right_fit = self.previous_right_lane_lines.get_smoothed_polynomial()\n",
    "            right_line.polynomial_coeff = right_fit\n",
    "            self.previous_right_lane_lines.append(right_line, force=True)\n",
    "            print(\"**** REVISED Poly right {0}\".format(right_fit))\n",
    "        #else:\n",
    "            #right_fit = self.previous_right_lane_lines.get_smoothed_polynomial()\n",
    "            #right_line.polynomial_coeff = right_fit\n",
    "\n",
    "\n",
    "    \n",
    "        # Generate x and y values for plotting\n",
    "        ploty = np.linspace(0, warped_img.shape[0] - 1, warped_img.shape[0] )\n",
    "        left_fitx = left_fit[0] * ploty**2 + left_fit[1] * ploty + left_fit[2]\n",
    "        right_fitx = right_fit[0] * ploty**2 + right_fit[1] * ploty + right_fit[2]\n",
    "        \n",
    "        \n",
    "        left_line.polynomial_coeff = left_fit\n",
    "        left_line.line_fit_x = left_fitx\n",
    "        left_line.non_zero_x = leftx  \n",
    "        left_line.non_zero_y = lefty\n",
    "\n",
    "        right_line.polynomial_coeff = right_fit\n",
    "        right_line.line_fit_x = right_fitx\n",
    "        right_line.non_zero_x = rightx\n",
    "        right_line.non_zero_y = righty\n",
    "\n",
    "        \n",
    "        return (left_line, right_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld = AdvancedLaneDetectorWithMemory(opts, ipts, src_pts, dst_pts, 20, 100, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_img = ld.process_image(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(proc_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(720, 1280, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_video_path = \"project_video.mp4\"\n",
    "challenge_video_path = \"challenge_video.mp4\"\n",
    "project_video_output_path = 'output_videos/lanes_project_video.mp4'\n",
    "project_video_sample_path = 'project_video_sample.mp4'\n",
    "challenge_video_sample_path = 'challenge_video_sample.mp4'\n",
    "\n",
    "project_video_sample_output_path = 'output_videos/lanes_project_video_sample.mp4'\n",
    "challenge_video_sample_output_path = 'output_videos/lanes_challenge_video_sample.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "ffmpeg_extract_subclip(project_video_path, 22, 27, targetname=project_video_sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = AdvancedLaneDetectorWithMemory(opts, ipts, src_pts, dst_pts, 20, 50, 10)\n",
    "\n",
    "clip1 = VideoFileClip(project_video_sample_path)\n",
    "project_video_clip = clip1.fl_image(detector.process_image) #NOTE: this function expects color images!!\n",
    "%time project_video_clip.write_videofile(project_video_sample_output_path, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"900\" height=\"600\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(project_video_sample_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = AdvancedLaneDetectorWithMemory(opts, ipts, src_pts, dst_pts, 20, 50, 10)\n",
    "\n",
    "video_clip_1 = VideoFileClip(project_video_path)\n",
    "project_video_clip = video_clip_1.fl_image(detector.process_image) #NOTE: this function expects color images!!\n",
    "%time project_video_clip.write_videofile(project_video_output_path, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"900\" height=\"600\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(project_video_output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challege Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "ffmpeg_extract_subclip(challenge_video_path, 0, 4, targetname=challenge_video_sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = AdvancedLaneDetectorWithMemory(opts, ipts, src_pts, dst_pts, 20, 100, 10)\n",
    "\n",
    "clip1 = VideoFileClip(challenge_video_sample_path)\n",
    "challenge_video_clip = clip1.fl_image(detector.process_image) #NOTE: this function expects color images!!\n",
    "%time challenge_video_clip.write_videofile(challenge_video_sample_output_path, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"900\" height=\"600\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(challenge_video_sample_output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We still have much work to do on the challenge videos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
